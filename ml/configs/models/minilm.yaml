model:
  name: nreimers/MiniLMv2-L12-H384-distilled-from-RoBERTa-Large # https://github.com/microsoft/unilm/tree/master/minilm
  max_length: 128

paths:
  artifacts_dir: artifacts/experiments/minilm_v1

training:
  goemotions:
    learning_rate: 3e-5
    train_batch_size: 32
    eval_batch_size: 32
    num_epochs: 3
    weight_decay: 0.01
    seed: 42
    warmup_ratio: 0.05
    label_smoothing_factor: 0.00
    early_stopping_patience: 2
    early_stopping_threshold: 0.0

  lemotif:
    common:
      train_batch_size: 32
      eval_batch_size: 32
      weight_decay: 0.01
      seed: 42
      gradient_accumulation_steps: 2
      intensity_loss_weight: 0.1
      metric_loss_weight: 0.1

    frozen:
      learning_rate: 1e-4
      num_epochs: 3
      warmup_ratio: 0.02
      early_stopping_patience: 2
      early_stopping_threshold: 0.0001
      lr_scheduler_type: linear

    unfrozen:
      learning_rate: 5e-5
      num_epochs: 25
      warmup_ratio: 0.05
      early_stopping_patience: 4
      early_stopping_threshold: 0.001
      lr_scheduler_type: cosine
      max_grad_norm: 1.0

evaluation:
  eval_batch_size: 128